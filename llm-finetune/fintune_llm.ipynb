{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import TrainingArguments, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, SFTTrainer\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing a dataset \n",
    "We are using a Huggingface hosted dataset consisting of Stackoverflow questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"llm-finetune/\"   # When running in lightning root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"\"   # when running from llm-finetune folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset ARGS:\n",
    "\n",
    "# ds_name = \"MaestroDmitry/stack-exchange-paired-shorted\"\n",
    "\n",
    "# https://huggingface.co/datasets/truthful_qa\n",
    "ds_name = \"truthful_qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface DPO trainer needs a dataset containing prompts, chosen, and rejected\n",
    "\n",
    "def return_prompts_and_responses(sample: Dict[str, List[str]|str], index) -> Dict[str, str]:\n",
    "    # prompts = [f\"Question: {question} \\n\\nAnswer: \" for question in sample[\"question\"]]\n",
    "    # prompt = f\"Question: {sample[\"question\"]} \\n\\nAnswer: \"\n",
    "    prompt = sample[\"question\"]\n",
    "\n",
    "    chosen = sample[\"correct_answers\"][min(index, len(sample[\"correct_answers\"])-1)]  # response_j\n",
    "    rejected = sample[\"incorrect_answers\"][min(index, len(sample[\"incorrect_answers\"])-1)]  # response_k\n",
    "\n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'chosen': chosen,\n",
    "        'rejected': rejected\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from Huggingface\n",
    "dataset = load_dataset(\n",
    "    ds_name, \"generation\",\n",
    "    cache_dir=folder+\"data\"\n",
    ")\n",
    "dataset = dataset['validation']\n",
    "\n",
    "train_dataset = dataset.map(\n",
    "    function=return_prompts_and_responses,\n",
    "    batched=False,\n",
    "    with_indices=False,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={\"index\": 0}\n",
    ")\n",
    "print(\"Train Dataset:\")\n",
    "print(train_dataset)\n",
    "print(train_dataset[0])\n",
    "\n",
    "test_dataset = dataset.map(\n",
    "    function=return_prompts_and_responses,\n",
    "    batched=False,\n",
    "    with_indices=False,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={\"index\": 1}\n",
    ")\n",
    "print(\"Test Dataset:\")\n",
    "print(test_dataset)\n",
    "print(test_dataset[0])\n",
    "\n",
    "eval_dataset = dataset.map(\n",
    "    function=return_prompts_and_responses,\n",
    "    batched=False,\n",
    "    with_indices=False,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={\"index\": 2}\n",
    ")\n",
    "print(\"Eval Dataset:\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a SFT base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n",
      "<|endoftext|>\n",
      "<|endoftext|>\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model_path = \"EleutherAI/gpt-neo-1.3B\"\n",
    "#lora_params = [\"q_proj\", \"v_proj\"]\n",
    "#batch_size = 2\n",
    "#m_name = \"neo-1.3B\"\n",
    "\n",
    "model_path = \"EleutherAI/gpt-neo-125m\"\n",
    "lora_params = [\"q_proj\", \"v_proj\"]\n",
    "batch_size = 12\n",
    "m_name = \"neo-125m\"\n",
    "\n",
    "# model_path = \"ComCom/gpt2-small\"\n",
    "# lora_params = [\"c_proj\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# load the base model in 4-bit quantization\n",
    "# TODO this only works on cuda\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',  # {\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    # use_auth_token=True,\n",
    "    cache_dir=folder+\"/model/\"+m_name+\"/base\"\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(tokenizer.eos_token_id))\n",
    "\n",
    "#if tokenizer.pad_token is None:\n",
    "#    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(tokenizer.pad_token)\n",
    "\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_string(q:str, a:str):\n",
    "    return f\"### Question: {q}\\n ### Answer: {a}\"\n",
    "\n",
    "def sft_formatting_func(example):\n",
    "    # method used for turning both good and bad example to supervised training data\n",
    "    output_texts = []\n",
    "    output_texts.append(qa_string(example['prompt'], example['chosen']))\n",
    "    output_texts.append(qa_string(example['prompt'], example['rejected']))\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of examples\n",
    "def print_some_samples(model):\n",
    "    text_list = [\n",
    "        \"what is the sun?\",\n",
    "        \"whats the difference between bash and zash?\",\n",
    "        \"How old i the city of Gothenburg?\",\n",
    "        \"When will i get to go home?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"model outputs:\")\n",
    "    print(\"----------------------------\")\n",
    "    for text in text_list:\n",
    "        # Tokenize text\n",
    "        text_ = qa_string(text, \"\")\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        inputs = inputs.to('cuda')\n",
    "        \n",
    "        # Run model\n",
    "        outputs = model.generate(input_ids=inputs, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, early_stopping=True, num_beams=1)  #  do_sample=True\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "        #f\"For question: \\n\"\n",
    "        # f\"{text} \\n\\n\"\n",
    "        print(f\"{decoded_output} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model outputs:\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the sun?\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The sun is a star.\n",
      "\n",
      "The \n",
      "\n",
      "\n",
      "whats the difference between bash and zash?\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a shell interpreter.\n",
      "\n",
      "A:\n",
      "\n",
      "zsh is a shell, bash is a \n",
      "\n",
      "\n",
      "How old i the city of Gothenburg?\n",
      "\n",
      "I am a young girl from Gothenburg, Sweden. I am a very happy person and I like to have fun. I like to go out and have fun with my friends. I like to go to the movies, to the theatre, to the concert, to the park, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach, to the beach \n",
      "\n",
      "\n",
      "When will i get to go home?\n",
      "\n",
      "I am a single mother of a 2 year old and a 1 year old. I have a 2 year old son and a 1 year old daughter. I have been in a relationship with my boyfriend for about a year and a half. I am not in a relationship with him. I am not in a relationship with anyone else. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with anyone. I am not in a relationship with \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_some_samples(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Lora args:\n",
    "lora_r = 8\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.0\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_params,  # related to choise of model, so model path / printed model\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_training_args: TrainingArguments = TrainingArguments(\n",
    "    output_dir=folder+\"/model/\"+m_name+\"/sft_train\",\n",
    "    num_train_epochs=500,\n",
    "    # logging_steps = 20,\n",
    "    # save_steps=10,  # defaults to 500\n",
    "    # use_cpu=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_gpu_eval_batch_size=batch_size,\n",
    "    logging_dir=folder+\"logs/sft_train\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    packing=True,  # Used only in case `dataset_text_field` is passed. This argument is used by the `ConstantLengthDataset` to pack the sequences of the dataset.\n",
    "    max_seq_length=512,  # The maximum sequence length to use for the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults to `512`.\n",
    "    formatting_func=sft_formatting_func,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_training_args,  # HF Trainer arguments\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# With gpt-neo-125 and qa dataset\n",
    "# on single A10-G = 25 min\n",
    "\n",
    "\n",
    "# These are old examples: when i was experimenting with the Stackoverflow dataset\n",
    "# ONLY RUNNING 3 EPOCHS!!\n",
    "\n",
    "# For: gpt-neo-1.3B\n",
    "# On single A10-G = 15h\n",
    "# On dual rtx 3090 = 8h\n",
    "\n",
    "# For: gpt-neo-125m\n",
    "# on single A10-G = TODO\n",
    "# on dual rtx 3090 = 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (q_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sft_model_path = folder+\"/model/\"+m_name+\"/sft_train/checkpoint-4000\"  # <- TODO\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    sft_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',  # {\"\": 0},\n",
    ")\n",
    "print(sft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model outputs:\n",
      "----------------------------\n",
      "what is the sun?\n",
      "- 0.00003\n",
      " \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats the difference between bash and zash?\n",
      "<jrib> zash: yes, but you can't say that bash is better\n",
      "<jrib> zash: but you can say that zash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you can say that bash is better\n",
      "<jrib> zash: but you \n",
      "\n",
      "\n",
      "How old i the city of Gothenburg?\n",
      "\n",
      "I am a city of Gothenburg, Sweden. I live in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a car in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a car in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a house in Gothenburg, Sweden. I have a \n",
      "\n",
      "\n",
      "When will i get to go home?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do?\n",
      "What do i do? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_some_samples(sft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_beta: float = 0.1\n",
    "dpo_training_args: Optional[TrainingArguments] = TrainingArguments(\n",
    "    output_dir=folder+\"/model/\"+m_name+\"/dpo_train\",\n",
    "    # use_cpu=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_gpu_eval_batch_size=batch_size,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=500,\n",
    "    # logging_steps = 200,\n",
    ")\n",
    "\n",
    "dpo_model = folder+\"/model/\"+m_name+\"/sft_train/checkpoint-4000\"  # <- TODO\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    dpo_model,  # location of saved SFT model\n",
    "    device_map='auto',  # {\"\": 0},\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    is_trainable=True,\n",
    ")\n",
    "\n",
    "dpo_model_ref = folder+\"/model/\"+m_name+\"/sft_train/checkpoint-4000\"  # <- TODO\n",
    "model_ref = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    dpo_model_ref,  # same model as the main one\n",
    "    device_map='auto',  # {\"\": 0},\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=dpo_training_args,\n",
    "    beta=dpo_beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.train()\n",
    "# dpo_trainer.save_model()\n",
    "\n",
    "# With gpt-neo-125 and qa dataset\n",
    "# on single A10-G = 50 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_model_path = folder+\"/model/\"+m_name+\"/dpo_train/checkpoint-500\"  # <- TODO\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    dpo_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto'  # {\"\": 0},\n",
    ")\n",
    "# print(dpo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model outputs:\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the sun?\n",
      "True No.\n",
      "# The sun is not a planet?\n",
      "# question#\n",
      " No# The sun is not a planet?\n",
      "# The question is not the sun?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not a planet?\n",
      "# The only planet in the solar system that has no moon is not \n",
      "\n",
      "\n",
      "whats the difference between bash and zash?\n",
      "True No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No No \n",
      "\n",
      "\n",
      "How old i the city of Gothenburg?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "?\n",
      "? \n",
      "\n",
      "\n",
      "When will i get to go home?\n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_some_samples(dpo_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
