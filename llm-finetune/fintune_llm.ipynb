{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import TrainingArguments, BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, SFTTrainer\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing a dataset \n",
    "We are using a Huggingface hosted dataset consisting of Stackoverflow questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"llm-finetune/\"   # When running in lightning root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"\"   # when running from llm-finetune folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset ARGS:\n",
    "\n",
    "# ds_name = \"MaestroDmitry/stack-exchange-paired-shorted\"\n",
    "\n",
    "# https://huggingface.co/datasets/truthful_qa\n",
    "ds_name = \"truthful_qa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface DPO trainer needs a dataset containing prompts, chosen, and rejected\n",
    "\n",
    "def return_prompts_and_responses(sample: Dict[str, List[str]|str], index) -> Dict[str, str]:\n",
    "    # prompts = [f\"Question: {question} \\n\\nAnswer: \" for question in sample[\"question\"]]\n",
    "    # prompt = f\"Question: {sample[\"question\"]} \\n\\nAnswer: \"\n",
    "    prompt = sample[\"question\"]\n",
    "\n",
    "    chosen = sample[\"correct_answers\"][min(index, len(sample[\"correct_answers\"])-1)]  # response_j\n",
    "    rejected = sample[\"incorrect_answers\"][min(index, len(sample[\"incorrect_answers\"])-1)]  # response_k\n",
    "\n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'chosen': chosen,\n",
    "        'rejected': rejected\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from Huggingface\n",
    "dataset = load_dataset(\n",
    "    ds_name, \"generation\",\n",
    "    cache_dir=folder+\"data\"\n",
    ")\n",
    "dataset = dataset['validation']\n",
    "\n",
    "train_dataset = dataset.map(\n",
    "    function=return_prompts_and_responses,\n",
    "    batched=False,\n",
    "    with_indices=False,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={\"index\": 0}\n",
    ")\n",
    "print(\"Train Dataset:\")\n",
    "print(train_dataset)\n",
    "print(train_dataset[0])\n",
    "\n",
    "test_dataset = dataset.map(\n",
    "    function=return_prompts_and_responses,\n",
    "    batched=False,\n",
    "    with_indices=False,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={\"index\": 1}\n",
    ")\n",
    "print(\"Test Dataset:\")\n",
    "print(test_dataset)\n",
    "print(test_dataset[0])\n",
    "\n",
    "eval_dataset = dataset.map(\n",
    "    function=return_prompts_and_responses,\n",
    "    batched=False,\n",
    "    with_indices=False,\n",
    "    remove_columns=dataset.column_names,\n",
    "    fn_kwargs={\"index\": 2}\n",
    ")\n",
    "print(\"Eval Dataset:\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a SFT base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"EleutherAI/gpt-neo-1.3B\"\n",
    "# batch_size = 2\n",
    "\n",
    "model_path = \"EleutherAI/gpt-neo-125m\"\n",
    "lora_params = [\"q_proj\", \"v_proj\"]\n",
    "batch_size = 12\n",
    "\n",
    "# model_path = \"ComCom/gpt2-small\"\n",
    "# lora_params = [\"c_proj\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# load the base model in 4-bit quantization\n",
    "# TODO this only works on cuda\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',  # {\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    # use_auth_token=True,\n",
    "    cache_dir=folder+\"model/base\"\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(tokenizer.eos_token_id))\n",
    "\n",
    "#if tokenizer.pad_token is None:\n",
    "#    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(tokenizer.pad_token)\n",
    "\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_string(q:str, a:str):\n",
    "    return f\"### Question: {q}\\n ### Answer: {a}\"\n",
    "\n",
    "def sft_formatting_func(example):\n",
    "    # method used for turning both good and bad example to supervised training data\n",
    "    output_texts = []\n",
    "    output_texts.append(qa_string(example['prompt'], example['chosen']))\n",
    "    output_texts.append(qa_string(example['prompt'], example['rejected']))\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of examples\n",
    "def print_some_samples(model):\n",
    "    text_list = [\n",
    "        \"what is the sun?\",\n",
    "        \"whats the difference between bash and zash?\",\n",
    "        \"How old i the city of Gothenburg?\",\n",
    "        \"When will i get to go home?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"model outputs:\")\n",
    "    print(\"----------------------------\")\n",
    "    for text in text_list:\n",
    "        # Tokenize text\n",
    "        text_ = qa_string(text, \"\")\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        inputs = inputs.to('cuda')\n",
    "        \n",
    "        # Run model\n",
    "        outputs = model.generate(input_ids=inputs, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, early_stopping=True, num_beams=1)  #  do_sample=True\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "        #f\"For question: \\n\"\n",
    "        # f\"{text} \\n\\n\"\n",
    "        print(f\"{decoded_output} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_some_samples(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Lora args:\n",
    "lora_r = 8\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.0\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_params,  # related to choise of model, so model path / printed model\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_training_args: TrainingArguments = TrainingArguments(\n",
    "    output_dir=folder+\"model/sft_train\",\n",
    "    num_train_epochs=500,\n",
    "    # logging_steps = 20,\n",
    "    # save_steps=10,  # defaults to 500\n",
    "    # use_cpu=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_gpu_eval_batch_size=batch_size,\n",
    "    logging_dir=folder+\"logs/sft_train\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=peft_config,\n",
    "    packing=True,  # Used only in case `dataset_text_field` is passed. This argument is used by the `ConstantLengthDataset` to pack the sequences of the dataset.\n",
    "    max_seq_length=512,  # The maximum sequence length to use for the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults to `512`.\n",
    "    formatting_func=sft_formatting_func,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_training_args,  # HF Trainer arguments\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# With gpt-neo-125 and qa dataset\n",
    "# on single A10-G = 25 min\n",
    "\n",
    "\n",
    "# These are old examples: when i was experimenting with the Stackoverflow dataset\n",
    "# ONLY RUNNING 3 EPOCHS!!\n",
    "\n",
    "# For: gpt-neo-1.3B\n",
    "# On single A10-G = 15h\n",
    "# On dual rtx 3090 = 8h\n",
    "\n",
    "# For: gpt-neo-125m\n",
    "# on single A10-G = TODO\n",
    "# on dual rtx 3090 = 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model_path = folder+\"model/sft_train/checkpoint-4000\"  # <- TODO\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    sft_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto',  # {\"\": 0},\n",
    ")\n",
    "print(sft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_some_samples(sft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_beta: float = 0.1\n",
    "dpo_training_args: Optional[TrainingArguments] = TrainingArguments(\n",
    "    output_dir=folder+\"model/dpo_train\",\n",
    "    # use_cpu=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_gpu_eval_batch_size=batch_size,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=500,\n",
    "    # logging_steps = 200,\n",
    ")\n",
    "\n",
    "dpo_model = folder+\"model/sft_train/checkpoint-4000\"  # <- TODO\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    dpo_model,  # location of saved SFT model\n",
    "    device_map='auto',  # {\"\": 0},\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    is_trainable=True,\n",
    ")\n",
    "\n",
    "dpo_model_ref = folder+\"model/sft_train/checkpoint-4000\"  # <- TODO\n",
    "model_ref = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    dpo_model_ref,  # same model as the main one\n",
    "    device_map='auto',  # {\"\": 0},\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=dpo_training_args,\n",
    "    beta=dpo_beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.train()\n",
    "# dpo_trainer.save_model()\n",
    "\n",
    "# With gpt-neo-125 and qa dataset\n",
    "# on single A10-G = 50 min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_model_path = folder+\"model/dpo_train/checkpoint-500\"  # <- TODO\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    dpo_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map='auto'  # {\"\": 0},\n",
    ")\n",
    "# print(dpo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_some_samples(dpo_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
