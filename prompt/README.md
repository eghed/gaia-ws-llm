# Run model locally

1. Install Ollama locally (ollama.com)
2. Run the following command in the terminal:
```ollama pull llama3```
3. ```ollama run llama3```